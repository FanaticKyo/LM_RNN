{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchnlp.nn as nlp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import *\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tests import test_prediction, test_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all that we need\n",
    "dataset = np.load('../dataset/wiki.train.npy', allow_pickle=True)\n",
    "fixtures_pred = np.load('../fixtures/prediction.npz', allow_pickle=True)  # dev\n",
    "fixtures_gen = np.load('../fixtures/generation.npy', allow_pickle=True)  # dev\n",
    "fixtures_pred_test = np.load('../fixtures/prediction_test.npz', allow_pickle=True)  # test\n",
    "fixtures_gen_test = np.load('../fixtures/generation_test.npy', allow_pickle=True)  # test\n",
    "vocab = np.load('../dataset/vocab.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sentence_level_dataset = []\n",
    "# for article in dataset:\n",
    "#     eos = np.where(article == 1417)[0] + 1\n",
    "#     sentences = np.split(article, eos)\n",
    "#     sentence_level_dataset.extend(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_level_dataset = []\n",
    "seq_len = 300\n",
    "p = 0.95\n",
    "for article in dataset:\n",
    "    num = article.shape[0]\n",
    "    i = 0\n",
    "    eos = []\n",
    "    for n in range(num // 150):\n",
    "        if i < num:\n",
    "            pp = np.random.uniform(0, 1)\n",
    "            if pp < p:\n",
    "                eos.append(i + seq_len)\n",
    "                i += seq_len\n",
    "            else:\n",
    "                eos.append(i + seq_len // 2)\n",
    "                i += seq_len // 2\n",
    "    eos = np.asarray(eos)\n",
    "    sentences = np.split(article, eos)\n",
    "    sentence_level_dataset.extend(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for sen in sentence_level_dataset:\n",
    "    if sen.shape[0] > 1:\n",
    "        dataset.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.asarray(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "class LanguageModelDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        # concatenate your articles and build into batches\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.dataset)\n",
    "        bs = self.batch_size\n",
    "        num_batch = self.dataset.shape[0] // bs\n",
    "        for i in range(num_batch):\n",
    "            batch_data = self.dataset[i:i+bs]\n",
    "#             inputs = [torch.LongTensor(line[:-1]) for line in batch_data]\n",
    "#             targets = [torch.LongTensor(line[1:]) for line in batch_data]\n",
    "            yield ([torch.LongTensor(line[:-1]) for line in batch_data], [torch.LongTensor(line[1:]) for line in batch_data])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "        TODO: Define your model here\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, nlayers):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers=nlayers\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size) # Embedding layer\n",
    "        self.rnn = nn.LSTM(input_size = embed_size,hidden_size=hidden_size,num_layers=nlayers,dropout=0.2) # Recurrent network\n",
    "        self.scoring = nn.Linear(hidden_size,vocab_size) # Projection layer\n",
    "        self.scoring.weight = self.embedding.weight\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    def forward(self, x):\n",
    "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
    "        batch_size = len(x)\n",
    "        inp = [i for i in x]\n",
    "        for idx, sentence in enumerate(inp):\n",
    "            inp[idx] = self.dropout(self.embedding(sentence))\n",
    "            \n",
    "        embed = pack_sequence(inp,enforce_sorted=False)\n",
    "        hidden = None\n",
    "        output_lstm,hidden = self.rnn(embed,hidden) #L x N x H\n",
    "        output_lstm,length = pad_packed_sequence(output_lstm)\n",
    "        output_lstm_no_pad = []\n",
    "        for idx in range(batch_size):\n",
    "            output_lstm_no_pad.append(output_lstm[:,idx,:][:length[idx]])\n",
    "        output_lstm_flatten = torch.cat(output_lstm_no_pad)\n",
    "        output_flatten = self.scoring(output_lstm_flatten) #(L*N) x V\n",
    "        result = output_flatten.argmax(dim=1)\n",
    "        output = []\n",
    "        idx = 0\n",
    "        for i in range(batch_size):\n",
    "            output.append(output_flatten[idx:idx+length[i]])\n",
    "            idx = idx+length[i]\n",
    "        out = []\n",
    "        idx = 0\n",
    "        for i in range(batch_size):\n",
    "            out.append(result[idx:idx+length[i]])\n",
    "            idx = idx+length[i]\n",
    "        return output_flatten, output, out\n",
    "\n",
    "#         return output_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model trainer\n",
    "\n",
    "class LanguageModelTrainer:\n",
    "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "        self.predictions_test = []\n",
    "        self.generated_logits = []\n",
    "        self.generated = []\n",
    "        self.generated_logits_test = []\n",
    "        self.generated_test = [] \n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        # TODO: Define your optimizer and criterion here\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=0.001)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.criterion = self.criterion.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
    "            loss, time = self.train_batch(inputs, targets)\n",
    "            epoch_loss += loss\n",
    "            print('Time:', time, 'Loss:', loss, end='\\r')\n",
    "            torch.cuda.empty_cache()\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1\n",
    "        print('\\n[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        \"\"\" \n",
    "            TODO: Define code for training a single batch of inputs\n",
    "        \n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        inputs = [i.cuda() for i in inputs]\n",
    "        targets = torch.cat(targets).cuda()\n",
    "        outputs, _, _ = self.model.forward(inputs)\n",
    "        batch_loss = 0\n",
    "        batch_loss = self.criterion(outputs, targets)\n",
    "#         loss = criterion(outputs.view(-1,outputs.size(2)),targets.view(-1)) # Loss of the flattened outputs\n",
    "        self.optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        end = time.time()\n",
    "        return batch_loss.item(), end - start\n",
    "    \n",
    "    def test(self):\n",
    "        # don't change these\n",
    "        self.model.eval() # set to eval mode\n",
    "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
    "        self.predictions.append(predictions)\n",
    "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
    "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
    "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
    "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
    "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
    "        self.val_losses.append(nll)\n",
    "        \n",
    "        self.generated.append(generated)\n",
    "        self.generated_test.append(generated_test)\n",
    "        self.generated_logits.append(generated_logits)\n",
    "        self.generated_logits_test.append(generated_logits_test)\n",
    "        \n",
    "        # generate predictions for test data\n",
    "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
    "        self.predictions_test.append(predictions_test)\n",
    "            \n",
    "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, nll))\n",
    "        return nll\n",
    "\n",
    "    def save(self):\n",
    "        # don't change these\n",
    "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.model.state_dict()},\n",
    "            model_path)\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated_test[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestLanguageModel:\n",
    "    def prediction(inp, model):\n",
    "        \"\"\"\n",
    "            TODO: write prediction code here\n",
    "            \n",
    "            :param inp:\n",
    "            :return: a np.ndarray of logits\n",
    "        \"\"\"\n",
    "        num_seq = inp.shape[0]\n",
    "        \n",
    "        sequence = [torch.LongTensor(inp[i]).cuda() for i in range(num_seq)]\n",
    "        out_flatten, out, _ = model.forward(sequence)\n",
    "        pred = [out[i][-1].detach().cpu() for i in range(len(out))]\n",
    "        pred = np.array(torch.stack(pred))\n",
    "        return pred\n",
    "\n",
    "        \n",
    "    def generation(inp, forward, model):\n",
    "        \"\"\"\n",
    "            TODO: write generation code here\n",
    "\n",
    "            Generate a sequence of words given a starting sequence.\n",
    "            :param inp: Initial sequence of words (batch size, length)\n",
    "            :param forward: number of additional words to generate\n",
    "            :return: generated words (batch size, forward)\n",
    "        \"\"\"        \n",
    "        num_seq = inp.shape[0]\n",
    "        \n",
    "        sequence = [torch.LongTensor(inp[i]).cuda() for i in range(num_seq)]\n",
    "        out_flatten, _, out = model.forward(sequence)\n",
    "        \n",
    "        next_word = [out[i][-1].view(1) for i in range(len(out))]\n",
    "        generation = next_word\n",
    "        new_sequence = [torch.cat([sequence[i], next_word[i]]) for i in range(len(sequence))]\n",
    "\n",
    "        for n_word in range(forward-1):\n",
    "            out_flatten, _, out = model.forward(new_sequence)\n",
    "            next_word = [out[i][-1].view(1) for i in range(len(out))]\n",
    "            new_sequence = [torch.cat([new_sequence[i], next_word[i]]) for i in range(len(sequence))]\n",
    "            generation = [torch.cat([generation[i].cpu(), next_word[i].cpu()]) for i in range(len(generation))]\n",
    "        generation = torch.stack(generation).numpy()\n",
    "        return generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define other hyperparameters here\n",
    "\n",
    "NUM_EPOCHS = 8\n",
    "BATCH_SIZE = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models, predictions, and generated words to ./experiments/1574409843\n"
     ]
    }
   ],
   "source": [
    "run_id = str(int(time.time()))\n",
    "if not os.path.exists('./experiments'):\n",
    "    os.mkdir('./experiments')\n",
    "os.mkdir('./experiments/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(len(vocab),512,512,4)\n",
    "model = model.cuda()\n",
    "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.08829855918884277 Loss: 6.2655134201049805\n",
      "[TRAIN]  Epoch [2/8]   Loss: 6.5995\n",
      "[VAL]  Epoch [2/8]   Loss: 5.7240\n",
      "Saving model, predictions and generated output for epoch 0 with NLL: 5.7240357\n",
      "Time: 0.08940696716308594 Loss: 6.0140638351440435\n",
      "[TRAIN]  Epoch [3/8]   Loss: 5.9849\n",
      "[VAL]  Epoch [3/8]   Loss: 5.3289\n",
      "Saving model, predictions and generated output for epoch 1 with NLL: 5.3289375\n",
      "Time: 0.0844113826751709 Loss: 5.662783145904541455.7058186531066895\n",
      "[TRAIN]  Epoch [4/8]   Loss: 5.7065\n",
      "[VAL]  Epoch [4/8]   Loss: 5.1974\n",
      "Saving model, predictions and generated output for epoch 2 with NLL: 5.197402\n",
      "Time: 0.08645439147949219 Loss: 5.5339198112487795\n",
      "[TRAIN]  Epoch [5/8]   Loss: 5.5238\n",
      "[VAL]  Epoch [5/8]   Loss: 4.8722\n",
      "Saving model, predictions and generated output for epoch 3 with NLL: 4.872234\n",
      "Time: 0.08043622970581055 Loss: 5.35037994384765655.412330627441406\n",
      "[TRAIN]  Epoch [6/8]   Loss: 5.3770\n",
      "[VAL]  Epoch [6/8]   Loss: 4.7845\n",
      "Saving model, predictions and generated output for epoch 4 with NLL: 4.78446\n",
      "Time: 0.0804600715637207 Loss: 5.24590587615966875\n",
      "[TRAIN]  Epoch [7/8]   Loss: 5.2706\n",
      "[VAL]  Epoch [7/8]   Loss: 4.7965\n",
      "Time: 0.0891566276550293 Loss: 5.28431129455566495Loss: 5.069775104522705\n",
      "[TRAIN]  Epoch [8/8]   Loss: 5.1483\n",
      "[VAL]  Epoch [8/8]   Loss: 4.7584\n",
      "Saving model, predictions and generated output for epoch 6 with NLL: 4.7583942\n",
      "Time: 0.0895543098449707 Loss: 4.97879409790039195\n",
      "[TRAIN]  Epoch [9/8]   Loss: 5.1239\n",
      "[VAL]  Epoch [9/8]   Loss: 4.6037\n",
      "Saving model, predictions and generated output for epoch 7 with NLL: 4.603655\n"
     ]
    }
   ],
   "source": [
    "best_nll = 1e30 \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    trainer.train()\n",
    "    nll = trainer.test()\n",
    "    if nll < best_nll:\n",
    "        best_nll = nll\n",
    "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
    "        trainer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.08361434936523438 Loss: 5.3015861511230475\n",
      "[TRAIN]  Epoch [10/8]   Loss: 5.0323\n",
      "[VAL]  Epoch [10/8]   Loss: 4.6337\n",
      "Time: 0.08559441566467285 Loss: 4.9341535568237305\n",
      "[TRAIN]  Epoch [11/8]   Loss: 4.9404\n",
      "[VAL]  Epoch [11/8]   Loss: 4.6186\n",
      "Time: 0.07928133010864258 Loss: 4.8148989677429215\n",
      "[TRAIN]  Epoch [12/8]   Loss: 4.9122\n",
      "[VAL]  Epoch [12/8]   Loss: 4.6415\n",
      "Time: 0.08373284339904785 Loss: 4.8244585990905765\n",
      "[TRAIN]  Epoch [13/8]   Loss: 4.8611\n",
      "[VAL]  Epoch [13/8]   Loss: 4.5575\n",
      "Saving model, predictions and generated output for epoch 3 with NLL: 4.5575223\n",
      "Time: 0.08637189865112305 Loss: 4.6906929016113285\n",
      "[TRAIN]  Epoch [14/8]   Loss: 4.7729\n",
      "[VAL]  Epoch [14/8]   Loss: 4.4475\n",
      "Saving model, predictions and generated output for epoch 4 with NLL: 4.4474525\n",
      "Time: 0.08555173873901367 Loss: 4.9631700515747075\n",
      "[TRAIN]  Epoch [15/8]   Loss: 4.7787\n",
      "[VAL]  Epoch [15/8]   Loss: 4.4532\n",
      "Time: 0.0842139720916748 Loss: 4.69238233566284265\n",
      "[TRAIN]  Epoch [16/8]   Loss: 4.7423\n",
      "[VAL]  Epoch [16/8]   Loss: 4.5161\n",
      "Time: 0.0869443416595459 Loss: 4.72446870803833175\n",
      "[TRAIN]  Epoch [17/8]   Loss: 4.6571\n",
      "[VAL]  Epoch [17/8]   Loss: 4.3947\n",
      "Saving model, predictions and generated output for epoch 7 with NLL: 4.3947377\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    trainer.train()\n",
    "    nll = trainer.test()\n",
    "    if nll < best_nll:\n",
    "        best_nll = nll\n",
    "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
    "        trainer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(),lr=0.001, weight_decay=1e-6)\n",
    "# criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for batch_num, (inputs, targets) in enumerate(loader):\n",
    "#     inputs = [i.cuda() for i in inputs]\n",
    "# #     targets = torch.cat(targets).cuda()\n",
    "#     outputs, out = model.forward(inputs)\n",
    "#     break\n",
    "# #     loss = criterion(outputs, targets)\n",
    "# #     optimizer.zero_grad()\n",
    "# #     loss.backward()\n",
    "# #     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Don't change these\n",
    "# plot training curves\n",
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input | Output #0: while the group was en route , but only three were ultimately able to attack . None of them were | the most common and most common of the United States\n",
      "Input | Output #1: <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = = <eol> In the United States , the\n",
      "Input | Output #2: 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | single , and the second single from the album was\n",
      "Input | Output #3: , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | <unk> , <unk> , <unk> , <unk> , <unk> ,\n",
      "Input | Output #4: Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | position in the United States . The <unk> was also\n",
      "Input | Output #5: the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | was awarded the <unk> Prize of the Year . The\n",
      "Input | Output #6: <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | to the west . The <unk> was the first to\n",
      "Input | Output #7: The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . <eol> = = = = <unk> = = =\n",
      "Input | Output #8: by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | the public . The following day , the company was\n",
      "Input | Output #9: Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . The French and Ottoman brigades were sent to the\n",
      "Input | Output #10: = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | 1944 , the battalion was ordered to withdraw and the\n",
      "Input | Output #11: of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a great part of the world 's most iconic ,\n",
      "Input | Output #12: — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Input | Output #13: And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the United States . The film was a commercial success\n",
      "Input | Output #14: Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | February 2009 , was a member of the National Academy\n",
      "Input | Output #15: Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | <unk> , a <unk> <unk> , and a <unk> <unk>\n",
      "Input | Output #16: Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | was a <unk> . <eol> = = = = <unk>\n",
      "Input | Output #17: by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | = = = Certifications = = = <eol> = =\n",
      "Input | Output #18: alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ 000 tonnes ) , and the <unk> ( <unk>\n",
      "Input | Output #19: while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the French fleet was the most active and most of\n",
      "Input | Output #20: first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | from the <unk> <unk> , which is a <unk> ,\n",
      "Input | Output #21: Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 3 @.@ 1 ft ) wide . The oribi is\n",
      "Input | Output #22: \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | United States is a <unk> , and the <unk> of\n",
      "Input | Output #23: were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the United States . The <unk> was a <unk> ,\n",
      "Input | Output #24: port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | . <eol> = = Meteorological history = = <eol> The\n",
      "Input | Output #25: T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | Simon , the highest of which was the strongest storm\n",
      "Input | Output #26: South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were also named the most successful player to win the\n",
      "Input | Output #27: Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The Portuguese was the first to be\n",
      "Input | Output #28: deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . <eol> = = = Reviews = = =\n",
      "Input | Output #29: , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . The first two @-@ lane , <unk> <unk> ,\n",
      "Input | Output #30: production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that the film was a \" <unk> \" . <eol>\n",
      "Input | Output #31: and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | . The song was a commercial success , and was\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see generated output\n",
    "print (trainer.generated[-1]) # get last generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1574409843'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp experiments/1574406970/predictions-test-10.npy predictions.npy\n",
    "!cp experiments/1574406970/generated-10.txt generated.txt\n",
    "!cp experiments/1574406970/generated_logits-test-10.npy generated_logits.npy\n",
    "!tar -cvf handin.tar training.ipynb predictions.npy generated.txt generated_logits.npy\n",
    "# !rm -f generated.txt predictions.npy training.ipynb generated_logits.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
